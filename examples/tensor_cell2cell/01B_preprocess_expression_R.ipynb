{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106218d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching SeuratObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(Seurat)\n",
    "library(SeuratDisk)\n",
    "library(sva) # only if using combat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e5a63",
   "metadata": {},
   "source": [
    "Load the BALF COVID dataset, which is described here: https://doi.org/10.1038/s41591-020-0901-9.\n",
    "\n",
    "This dataset contains 12 samples, each associated with \"Healthy Control\", \"Moderate\", or \"Severe\" COVID contexts. \n",
    "\n",
    "You can download the 12 scRNAseq .h5 files under the samples section here: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE145926\n",
    "You can also download the metadata file: TODO\n",
    "\n",
    "Once loaded, we begin with a basic preprocessing of each sample based on QC metrics. See https://satijalab.org/seurat/articles/pbmc3k_tutorial.html for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80279064",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression.data.path<-'/data2/eric/Tensor-Revisions/data/COVID-19/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a327438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the metadata\n",
    "md <- read.table(paste0(expression.data.path, 'metadata.txt'), header = T, row.names = 'ID')\n",
    "colnames(md) = c('Sample.ID', 'sample_new', 'Context', 'disease', 'hasnCoV', 'cluster', 'cell.type')\n",
    "\n",
    "context.map = c('Healthy.Control', 'Moderate.Covid', 'Severe.Covid')\n",
    "names(context.map) <- c('HC', 'M', 'S')\n",
    "md['Context'] <- unname(context.map[md$Context])\n",
    "\n",
    "balf.samples<-list()\n",
    "for (filename in list.files(expression.data.path)){\n",
    "    if (endsWith(filename, '.h5')){\n",
    "        sample<-unlist(strsplit(filename, '_'))[[2]]\n",
    "        \n",
    "        # subset and format metadata\n",
    "        md.sample<-md[md$Sample.ID == sample,]\n",
    "        rownames(md.sample) <- unname(sapply(rownames(md.sample), \n",
    "                                           function(x) paste0(unlist(strsplit(x, '_'))[[1]], '-1')))\n",
    "        # load the counts\n",
    "        so <- Seurat::Read10X_h5(filename=paste0(expression.data.path, filename), unique.features=T)\n",
    "        so <- so[, rownames(md.sample)]\n",
    "                                           \n",
    "        # preprocess\n",
    "        so <- CreateSeuratObject(counts=so, project=sample, meta.data=md.sample[c('Sample.ID', 'Context', 'cell.type')], \n",
    "                      min.features=30, min.cells=3)\n",
    "        # filter cells based on QC metrics\n",
    "        so[[\"percent.mt\"]] <- PercentageFeatureSet(so, pattern = \"^MT-\")\n",
    "        so <- subset(so, subset = nFeature_RNA < 6000 & percent.mt < 10)\n",
    "        \n",
    "        balf.samples[[sample]] <- so                                   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf64480",
   "metadata": {},
   "source": [
    "Next, we normalize the raw UMI counts. We recommend log(1+CPM) normalization, as this maintains non-negative counts and is the input for many communication scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83b48da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "balf.samples <- lapply(balf.samples, \n",
    "                    function(so) NormalizeData(so, normalization.method = \"LogNormalize\", scale.factor = 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf8130",
   "metadata": {},
   "source": [
    "Finally, we apply a batch correction. The goal here is to account for sample-to-sample technical variability. \n",
    "\n",
    "At this point, we diverge from the Python preprocessing tutorial in order to leverage Seurat's built-in batch correction functions. To decrease run time, we will use reciprocal PCA instead of CCA. See https://satijalab.org/seurat/articles/integration_introduction.html and Seurat's other integration vignettes for additional details. To apply Combat as in scanpy, see commented code further below.\n",
    "\n",
    "Note, the final input matrices to Tensor-cell2cell must be non-negative. We will demonstrate workarounds to negative counts in the tensor building tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9cf54db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.var <- 'Sample.ID' # the batch variable in the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9f8006b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the HVGs for each sample separately\n",
    "balf.samples <- lapply(balf.samples, \n",
    "                      function(so) FindVariableFeatures(so, selection.method = \"vst\", nfeatures = 2000))\n",
    "                       \n",
    "# find the common HVGs across samples\n",
    "integration.features <- SelectIntegrationFeatures(object.list = balf.samples)\n",
    "\n",
    "                       \n",
    "# # to use CCA instead of reciprocal PCA, follow lines 10-12, instead of lines 14-22\n",
    "# # find the integration anchors\n",
    "# integration.anchors <- FindIntegrationAnchors(object.list = balf.samples, \n",
    "#                                               anchor.features = integration.features)    \n",
    "                       \n",
    "# calculate PCA on each sample separately\n",
    "balf.samples <- lapply(X = balf.samples, FUN = function(x) {\n",
    "    x <- ScaleData(x, features = integration.features, verbose = F)\n",
    "    x <- RunPCA(x, features = integration.features, verbose = F)\n",
    "})\n",
    "\n",
    "# find the integration anchors\n",
    "integration.anchors <- FindIntegrationAnchors(object.list = balf.samples, \n",
    "                                              anchor.features = integration.features, reduction = \"rpca\")\n",
    "\n",
    "# do the batch correction\n",
    "balf.corrected <- IntegrateData(anchorset = integration.anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "91daf6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000 top variable features were already calculated\n",
    "\n",
    "# get PCA to 100 PCs\n",
    "balf.corrected <- ScaleData(balf.corrected, verbose = F)\n",
    "balf.corrected <- RunPCA(balf.corrected, npcs = 100, verbose = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4395bdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "An object of class Seurat \n",
       "24900 features across 63102 samples within 2 assays \n",
       "Active assay: integrated (2000 features, 2000 variable features)\n",
       " 1 other assay present: RNA\n",
       " 1 dimensional reduction calculated: pca"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "balf.corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7bac1",
   "metadata": {},
   "source": [
    "Do the batch correction using Combat. This most closely emulated the steps taken in the scanpy tutorial, though there are still differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20ef0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge the balf samples\n",
    "\n",
    "# # note, this is less stringent than scanpy's concat method and will retain all features\n",
    "# balf.corrected<-merge(x = balf.samples[[1]], y = balf.samples[2:length(balf.samples)], \n",
    "#                       add.cell.ids = names(balf.samples), merge.data = TRUE)\n",
    "\n",
    "\n",
    "# # prepare batch covariate\n",
    "# batch<-balf.corrected@meta.data[[batch.var]]\n",
    "# names(batch)<-row.names(balf.corrected@meta.data)\n",
    "# batch<-as.factor(batch)\n",
    "\n",
    "# # do the batch correction\n",
    "# com <- ComBat(dat=balf.corrected@assays$RNA@data, batch=batch)\n",
    "\n",
    "# # store the batch corrected matrix in the scale.data slot\n",
    "# balf.corrected@assays$RNA@scale.data<-com\n",
    "\n",
    "# # get the top 2000 highly variable genes\n",
    "# balf.corrected<-FindVariableFeatures(balf.corrected, nfeatures=2000)\n",
    "\n",
    "# # get PCA to 100 PCs, calculated on batch corrected matrix\n",
    "# RunPCA(balf.corrected, features=VariableFeatures(balf.corrected), npcs=100, \n",
    "#        assay = 'RNA', slot = 'scale.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71570ba",
   "metadata": {},
   "source": [
    "Finally, we can convert this batch-correct Seurat object to an AnnData object using SeuratDisk (see https://mojaveazure.github.io/seurat-disk/articles/convert-anndata.html for details). The resultant h5ad file should contain the same information as the saved h5ad file in the Python tutorial at the end of tutorial 01A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c0e9be3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating h5Seurat file for version 3.1.5.9900\n",
      "\n",
      "Adding counts for RNA\n",
      "\n",
      "Adding data for RNA\n",
      "\n",
      "No variable features found for RNA\n",
      "\n",
      "No feature-level metadata found for RNA\n",
      "\n",
      "Adding data for integrated\n",
      "\n",
      "Adding scale.data for integrated\n",
      "\n",
      "Adding variable features for integrated\n",
      "\n",
      "No feature-level metadata found for integrated\n",
      "\n",
      "Adding cell embeddings for pca\n",
      "\n",
      "Adding loadings for pca\n",
      "\n",
      "No projected loadings for pca\n",
      "\n",
      "Adding standard deviations for pca\n",
      "\n",
      "No JackStraw data for pca\n",
      "\n",
      "Validating h5Seurat file\n",
      "\n",
      "Adding scale.data from integrated as X\n",
      "\n",
      "Adding data from integrated as raw\n",
      "\n",
      "Transfering meta.data to obs\n",
      "\n",
      "Adding dimensional reduction information for pca\n",
      "\n",
      "Adding feature loadings for pca\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.path = '/data3/hratch/c2c_general/'\n",
    "SaveH5Seurat(balf.corrected, filename = paste0(out.path, 'Rbatch_corrected_balf_covid.h5Seurat'))\n",
    "Convert(paste0(out.path, 'Rbatch_corrected_balf_covid.h5Seurat'), dest = \"h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b1138",
   "metadata": {},
   "source": [
    "The .h5ad file, named below, can be read into scanpy using scanpy.read_h5ad(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1333c4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"/data3/hratch/c2c_general/Rbatch_corrected_balf_covid.h5ad\"\n"
     ]
    }
   ],
   "source": [
    "filename=paste0(out.path, 'Rbatch_corrected_balf_covid.h5ad')\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:FDA_collab] *",
   "language": "R",
   "name": "conda-env-FDA_collab-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
